<html>
<head>
	<meta charset="utf-8">
	<title>Локальные нейронные сети</title>
	<link rel="icon" href="../image/Neural-Networks/neural-networks-favicon.png">
	<link rel="stylesheet" href="../html/main.css" type="text/css">	
	<link rel="stylesheet" href="../html/zoom.css" type="text/css">
	<link rel="stylesheet" href="../html/table.css" type="text/css">
	<link rel="stylesheet" href="../html/frame.css">
	<link rel="stylesheet" href="../html/ol-li.css" type="text/css">
	<script src="../html/jquery.2.2.0.min.js"></script>
	<script type="text/javascript" src="../html/litezoom.js"></script>
</head>
<body>
	<div class="site">
		<div class="header">
			<div id="logo">
				<p style="margin:10px; padding:0px;">Локальные нейронные сети</p>
			</div>
		</div>
		<div class="content">
			<center>
				<div class="images">
					<img src="../image/Neural-Networks/neural-networks-favicon.png" width="200px"/>
				</div>
				<div style="clear:both"></div>
			</center>
		</div>
		<div class="content">
			<p><a name="oglavlenie"></a></p>
			<center>
				<h1>Оглавление</h1>
			</center>
			<h2>Оглавление</h2>
			<ol>
				<li><a href="#part1">Нейронные сети на ПК</a></li>
				<ol>
					<li><a href="#part1.1">Програмное обеспечение</a></li>
					<ol>
						<li><a href="#part1.1.1">Llama-CPP</a></li>
						<li><a href="#part1.1.2">msty app</a></li>
						<li><a href="#part1.1.3">Jan for Desktop</a></li>
						<li><a href="#part1.1.4">open-webui на Linux</a></li>
						<li><a href="#part1.1.5">AnythingLLM</a></li>
						<li><a href="#part1.1.6">LM Studio</a></li>
						<li><a href="#part1.1.7">Appimage extract</a></li>
						<li><a href="#part1.1.8">Text-Generation-Webui</a></li>
						<li><a href="#part1.1.9">Miniconda3</a></li>
						<li><a href="#part1.1.10">TavernAI для Text-Generation-Webui</a></li>
						<li><a href="#part1.1.11">Stable-Diffusion-Webui</a></li>
						<li><a href="#part1.1.12">one-click-installers</a></li>
						<li><a href="#part1.1.13">KoboldAI или KoboldAI GIT</a></li>
						<li><a href="#part1.1.14">Krita</a></li>
						<li><a href="#part1.1.15">Krita AI Diffusion</a></li>
						<li><a href="#part1.1.16">SubTitleEdit</a></li>
						<li><a href="#part1.1.17">stabilityai/stable-diffusion-3.5-large-turbo</a></li>
						<li><a href="#part1.1.18">Image Creator</a></li>
						<li><a href="#part1.1.19">Stability Matrix</a></li>
						<li><a href="#part1.1.20">DALL·E mini</a></li>
					</ol>
					<li><a href="#part1.2">Параметры</a></li>
					<ol>
						<li><a href="#part1.2.1">VRAM и RAM для запуска модели</a></li>
						<li><a href="#part1.2.2">Квантизации и требования к оперативной памяти</a></li>
						<li><a href="#part1.2.3">Форматы с плавающими точками</a></li>
					</ol>
					<li><a href="#part1.3">Модели</a></li>
					<ol>
						<li><a href="#part1.3.1">Mykes/medicus</a></li>
						<li><a href="#part1.3.2">gemma3:4b</a></li>
						<li><a href="#part1.3.3">codegemma</a></li>
						<li><a href="#part1.3.4">codellama</a></li>
						<li><a href="#part1.3.5">dolphin-mistral:7b</a></li>
						<li><a href="#part1.3.6">dolphin-mixtral</a></li>
						<li><a href="#part1.3.7">llava</a></li>
						<li><a href="#part1.3.8">falcon</a></li>
						<li><a href="#part1.3.9">llama2-uncensored</a></li>
						<li><a href="#part1.3.10">reefer/erplegend</a></li>
						<li><a href="#part1.3.11">gdisney/neural-chat-uncensored</a></li>
						<li><a href="#part1.3.12">reefer/erphermesl3</a></li>
						<li><a href="#part1.3.13">jimscard/adult-film-screenwriter-nsfw</a></li>
						<li><a href="#part1.3.14">TheBloke/Llama-2-7B-GGUF</a></li>
						<li><a href="#part1.3.15">nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF</a></li>
						<li><a href="#part1.3.16">saiga2 7b gguf stable-diffusion model</a></li>
						<li><a href="#part1.3.17">PygmalionAI</a></li>
						<li><a href="#part1.3.18">Другие</a></li>
						<li><a href="#part1.3.20">Lora Model</a></li>
						<li><a href="#part1.3.21">DistilBERT</a></li>
						<li><a href="#part1.3.22">GPT-2 </a></li>
						<li><a href="#part1.3.23">BERT</a></li>
					</ol>
					<li><a href="#part1.4">Программирование</a></li>
					<li><a href="#part1.5">Дополнительная информация</a></li>
				</ol>
				<li><a href="#part2">Нейронные сети на Android / IOS</a></li>
				<ol>
					<li><a href="#part2.1">Интерфейсы</a></li>
					<li><a href="#part2.2">Параметры</a></li>
					<li><a href="#part2.3">Модели</a></li>
					<li><a href="#part2.4">Итоги</a></li>
				</ol>
			</ol>
		</div>
		<div class="content">
			<p><a name="part1"></a></p>
			<h2>1. Нейронные сети на ПК</h2>
			<p><b>Что такое нейросети, модели и квантизация?</b></p>
			<ul>
				<li><b>Нейросеть</b> — это математическая модель, вдохновлённая работой мозга, которая состоит из множества взаимосвязанных "нейронов" (узлов). Она обучается на данных, чтобы решать задачи: распознавать изображения, переводить текст, генерировать контент и т.д.</li>
				<li><b>Модель</b> — это конкретная обученная нейросеть с набором параметров (весов), которые она получила в процессе обучения. Например, модель GPT-2 — это нейросеть, обученная генерировать текст.</li>
				<li><b>Квантизация</b> — это способ уменьшить размер модели и ускорить её работу, переводя параметры из высокоточных чисел (например, 32-битных float) в более компактные форматы (например, 8-битные int). Это снижает требования к памяти и вычислениям, иногда с небольшой потерей качества, но часто незаметной для пользователя.</li>
			</ul>
			<p><b>Что такое LoRA (Low-Rank Adaptation)?</b></p>
			<ul>
				<li>LoRA — это метод <b>тонкой настройки (fine-tuning)</b> больших моделей, который позволяет адаптировать модель под новую задачу, обучая лишь небольшую часть параметров, а не всю модель целиком.</li>
				<li>Идея в том, что вместо изменения огромной матрицы весов модели, LoRA добавляет две маленькие матрицы низкого ранга, которые корректируют поведение модели. Это значительно снижает вычислительные затраты и объём данных для обучения, сохраняя при этом высокое качество результата.</li>
				<li>Такой подход особенно полезен для дообучения больших языковых моделей (LLM) и генеративных моделей, когда нет ресурсов для полного переобучения.</li>
			</ul>
			<p><a name="part1.1"></a></p>
			<h3>1.1. Програмное обеспечение</h3>
			<p>Официальный сайт <a href="https://ollama.com/">Ollama</a>.</p>
			<p><a name="part1.1.1"></a></p>
			<p>Python версия <a href="https://github.com/ggerganov/llama.cpp.git">Llama-CPP</a></p>
			<p><b>Llama-CPP</b> — минималистичный open-source движок для запуска больших языковых моделей (LLM) на CPU и GPU с поддержкой web-интерфейса и API.</p>
			<p>Установка и запуск.</p>
			<div class="codeses">
				<pre>git clone https://github.com/ggerganov/llama.cpp.git
pip install -r llama.cpp/requirements.txt
python llama.cpp/convert.py -h
python llama.cpp/convert.py vicuna-hf \ --outfile vicuna-13b-v1.5.gguf \ --outtype q8_0
# --outtype f16
# --outtype f32
# --outtype bf16</pre>
			</div>
			<p>Установка на Linux и использование.</p>
			<div class="codeses">
				<pre>$ ollama show your-model-exist:latest --modelfile > Modelfile
$ nano Modelfile # Example

# FROM Model-Name
FROM /YOUR/PATH/Local-Models/Model-Name.ext-model
TEMPLATE "<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"
SYSTEM You are my-own-model, a helpful AI assistant.
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>

$ ollama create my-own-model -f Modelfile
$ ollama run my-own-model</pre>
			</div>
			<p><a name="part1.1.2"></a></p>
			<p>Приложение <a href="https://msty.app/">msty app</a></p>
			<p><b>msty app</b> — приложение для работы с локальными языковыми моделями с удобным интерфейсом и поддержкой различных форматов моделей.</p>
			<ul>
				<li><a href="https://assets.msty.app/prod/latest/win/auto/Msty_x64.exe">msty-x64 windows gpu (amd or nvidia)</a></li>
				<li><a href="https://assets.msty.app/prod/latest/win/cpu/Msty_x64.exe">msty-x64 windows cpy only</a></li>
				<li><a href="https://assets.msty.app/prod/latest/linux/amd64/Msty_x86_64_amd64.AppImage">msty-x64 linux nvidia AppImage</a></li>
				<li><a href="https://assets.msty.app/prod/latest/linux/amd64/Msty_amd64_amd64.deb">msty-x64 linux nvidia DEB</a></li>
				<li><a href="https://assets.msty.app/prod/latest/linux/rocm/Msty_x86_64_rocm.AppImage">msty-x64 linux amd gpu -rocm AppImage</a></li>
				<li><a href="https://assets.msty.app/prod/latest/linux/rocm/Msty_amd64_rocm.deb">msty-x64 linux amd gpu -rocm DEB</a></li>
			</ul>
			<p><a name="part1.1.3"></a></p>
			<p>Приложение <a href="https://jan.ai/download">Jan for Desktop</a></p>
			<p><b>Jan for Desktop</b> — локальный AI-ассистент с возможностью работы с языковыми моделями и интеграцией в рабочий процесс.</p>
			<p><a name="part1.1.4"></a></p>
			<p>Использование <a href="https://docs.openwebui.com/">open-webui</a> на Linux.</p>
			<p><b>open-webui на Linux</b> — веб-интерфейс для запуска и управления локальными языковыми моделями на Linux-системах.</p>
			<div class="codeses">
				<pre>curl -LsSf https://astral.sh/uv/install.sh | sh
pip install open-webui
open-webui serve
pip install --upgrade open-webui</pre>
			</div>
			<p><a name="part1.1.5"></a></p>
			<p>Приложение <a href="https://anythingllm.com/">AnythingLLM</a></p>
			<p><b>AnythingLLM</b> — универсальный фронтенд для запуска и взаимодействия с различными локальными языковыми моделями.</p>
			<ul>
				<li><a href="https://cdn.useanything.com/latest/AnythingLLMDesktop-Silicon.dmg">AnythingLLM Desktop MacOS Silicon</a></li>
				<li><a href="https://cdn.useanything.com/latest/AnythingLLMDesktop.dmg">AnythingLLM Desktop MacOS</a></li>
				<li><a href="https://cdn.useanything.com/latest/AnythingLLMDesktop.exe">AnythingLLM Desktop Windows</a></li>
				<li><a href="https://cdn.useanything.com/latest/AnythingLLMDesktop-Arm64.exe">AnythingLLM Desktop Windows ARM64</a></li>
				<li><a href="https://docs.anythingllm.com/installation-desktop/linux#install-using-the-installer-script">Docs AnythingLLM</a></li>
				<li><a href="https://github.com/Mintplex-Labs/anything-llm">Mintplex-Labs AnythingLLM</a></li>
				<li><a href="https://github.com/Mintplex-Labs/anything-llm.git">Mintplex-Labs AnythingLLM GIT</a></li>
			</ul>
			<p><a name="part1.1.6"></a></p>
			<p>Приложение <a href="https://lmstudio.ai/">LM Studio</a></p>
			<p><b>LM Studio</b> — приложение для локального запуска и управления большими языковыми моделями с удобным графическим интерфейсом.</p>
			<ul>
				<li><a href="https://installers.lmstudio.ai/win32/x64/0.3.6-8/LM-Studio-0.3.6-8-x64.exe">LM-Studio Windows X64</a></li>
				<li><a href="https://installers.lmstudio.ai/linux/x64/0.3.6-8/LM-Studio-0.3.6-8-x64.AppImage">LM Studion AppImage</a></li>
			</ul>
			<p><a name="part1.1.7"></a></p>
			<p><b>Appimage extract</b> — код для распаковки и извлечения содержимого из любого AppImage-файлов на Linux.</p>
			<p>Appimage extract:</p>
			<div class="codeses">
				<pre>./Application_xxx-version.AppImage --appimage-extract
cat /usr/share/applications/Your-Apps_x-vers.desktop | grep -Ei "Categories"
nano ./squashfs-root/Application_xxx-version.desktop
chmod +x ./squashfs-root/AppRun
chmod +x ./squashfs-root/*.desktop
wget "https://github.com/AppImage/AppImageKit/releases/download/continuous/appimagetool-x86_64.AppImage"
chmod +x ./appimagetool-x86_64.AppImage
ARCH=x86_64 /appimagetool-x86_64.AppImage -n squashfs-root</pre>
			</div>
			<p>Несколько возможных причин, по которым LM Studio не загружает модель, и способы их устранения:</p>
			<ul>
				<li>Размер модели больше объёма VRAM. Если в LM Studio пытаются выгрузить модель на графический процессор, её размер должен быть меньше объёма VRAM, чтобы модель поместилась туда. Попробуйте установить значение «GPU offload» в 0 или отключить эту функцию.</li>
				<li>Включение функции «Keep entire model in RAM». Если её отключить, то можно запросить модель и получить ответ.</li>
				<li>Использование видеокарты для загрузки модели. Если на видеокарте достаточно памяти для модели, то можно попробовать использовать её. Для этого нужно кликнуть на галочку «GPU Offloading» и убедиться, что там стоит n_layers = -1</li>
			</ul>
			<p><a name="part1.1.8"></a></p>
			<p><a name="TextGenerationWebui"></a></p>
			<p>Приложение <a href="https://github.com/oobabooga/text-generation-webui">Text-Generation-Webui</a></p>
			<p><b>Text-Generation-Webui</b> — веб-интерфейс для генерации текста с использованием локальных языковых моделей, поддерживает множество моделей и кастомизацию.</p>
			<ul>
				<li><a href="https://github.com/oobabooga/text-generation-webui.git">Text-Generation-Webui Git https</a></li>
				<li><a href="git@github.com:oobabooga/text-generation-webui.git">Text-Generation-Webui Git git</a></li>
			</ul>
			<p>Linux запуск.</p>
			<div class="codeses">
				<pre>start_linux.sh --api --cpu
start_linux.sh --api --cuda
start_linux.sh --api</pre>
			</div>
			<p><a name="part1.1.9"></a></p>
			<p>Приложение <a href="https://repo.anaconda.com/miniconda/Miniconda3-py38_23.5.2-0-Linux-x86_64.sh">Miniconda3</a></p>
			<p><b>Miniconda3</b> — минималистичный дистрибутив Python с менеджером пакетов conda для управления окружениями и зависимостями.</p>
			<p>Запуск на Linux.</p>
			<div class="codeses">
				<pre>wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.5.2-0-Linux-x86_64.sh
sudo bash ./Miniconda3-py38_23.5.2-0-Linux-x86_64.sh
conda update -n base -c defaults conda</pre>
			</div>
			<p><a name="part1.1.10"></a></p>
			<p>Приложение <a href="https://github.com/TavernAI/TavernAI">TavernAI</a> для <a href="#TextGenerationWebui">Text-Generation-Webui</a></p>
			<p><b>TavernAI</b> для <u>Text-Generation-Webui</u> — расширение для Text-Generation-Webui, добавляющее удобный интерфейс для ролевых игр и чат-ботов.</p>
			<ul>
				<li><a href="https://sourceforge.net/projects/tavernaimain/files/">TavernAI sourceforge</a></li>
				<li><a href="https://github.com/TavernAI/TavernAI">TavernAI</a></li>
				<li><a href="https://github.com/TavernAI/TavernAI.git">TavernAI Git https</a></li>
				<li><a href="git@github.com:TavernAI/TavernAI.git">TavernAI Git git</a></li>
				<li><a href="https://github.com/TavernAI/TavernAI/releases">TavernAI Git Release</a></li>
			</ul>
			<p><a name="part1.1.11"></a></p>
			<p>Приложение <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Stable-Diffusion-Webui</a></p>
			<p><b>Stable-Diffusion-Webui</b> — популярный веб-интерфейс для локального запуска модели Stable Diffusion с множеством расширений и поддержкой LoRA.</p>
			<ul>
				<li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui.git">Stable-Diffusion-Webui Git https</a></li>
				<li><a href="git@github.com:AUTOMATIC1111/stable-diffusion-webui.git"></a>Stable-Diffusion-Webui Git git</li>
			</ul>
			<p><a name="part1.1.12"></a></p>
			<p>Приложение <a href="https://github.com/oobabooga/one-click-installers">one-click-installers</a></p>
			<p><b>one-click-installers</b> — скрипты и установщики для быстрого развёртывания Text-Generation-Webui и связанных инструментов.</p>
			<ul>
				<li><a href="https://github.com/oobabooga/one-click-installers.git">one-click-installers git https</a></li>
				<li><a href="git@github.com:oobabooga/one-click-installers.git">one-click-installers git git</a></li>
			</ul>
			<p><a name="part1.1.13"></a></p>
			<p>Приложение <a href="https://github.com/ebolam/KoboldAI">KoboldAI</a> или <a href="https://github.com/ebolam/KoboldAI.git">KoboldAI GIT</a></p>
			<p><b>KoboldAI</b> или <b>KoboldAI GIT</b> — платформа для интерактивного сторителлинга и генерации текста с поддержкой различных языковых моделей.</p>
			<ul>
				<li><a href="https://github.com/KoboldAI/KoboldAI-Client.git">KoboldAI-Client Git https</a></li>
				<li><a href="git@github.com:KoboldAI/KoboldAI-Client.git">KoboldAI-Client Git git</a></li>
				<li><a href="https://github.com/henk717/KoboldAI.git">KoboldAI United (Экспериментальная версия) Git https</a></li>
				<li><a href="git@github.com:henk717/KoboldAI.git">KoboldAI United (Экспериментальная версия) Git git</a></li>
				<li><a href="https://github.com/LostRuins/koboldcpp.git">KoboldAI CU Git https</a></li>
				<li><a href="git@github.com:LostRuins/koboldcpp.git">KoboldAI CU Git git</a></li>
				<li><a href="https://colab.research.google.com/github/koboldai/KoboldAI-Client/blob/main/colab/GPU.ipynb">Colab KoboldAI GPU</a></li>
				<li><a href="https://colab.research.google.com/github/KoboldAI/KoboldAI-Client/blob/main/colab/TPU.ipynb">Colab KoboldAI TPU</a></li>
				<p>Пока Google не исправит драйвера для TPU, эта версия бесполезна.</p>
			</ul>
			<p><a name="part1.1.14"></a></p>
			<p>Программа <a href="https://krita.org/en/download/">Krita</a></p>
			<p><b>Krita</b> — бесплатный и мощный редактор растровой графики, ориентированный на цифровую живопись и иллюстрацию.</p>
			<p>Например.</p>
			<ul>
				<li><a href="https://download.kde.org/stable/krita/5.2.9/krita-x64-5.2.9-setup.exe">krita-x64-5.2.9-setup.exe</a></li>
				<li><a href="https://download.kde.org/stable/krita/5.2.9/krita-x64-5.2.9.zip">krita-x64-5.2.9.zip</a></li>
				<li><a href="https://download.kde.org/stable/krita/5.2.9/krita-5.2.9-x86_64.AppImage">krita-5.2.9-x86_64.AppImage</a></li>
				<li><a href="https://download.kde.org/stable/krita/KritaShellExtension-v1.2.4b-setup.exe">KritaShellExtension-v1.2.4b-setup.exe</a></li>
			</ul>
			<p><a name="part1.1.15"></a></p>
			<p>Внедрение Diffusion в Krita.</p>
			<p>Приложение <a href="https://github.com/Acly/krita-ai-diffusion">Krita AI Diffusion</a></p>
			<p><b>Krita AI Diffusion</b> — плагин для Krita, позволяющий использовать модели диффузии для генерации и редактирования изображений.</p>
			<ul>
				<li><a href="https://github.com/Acly/krita-ai-diffusion/releases">Krita AI Diffusion Release</a></li>
				<li><a href="https://github.com/Acly/krita-ai-diffusion/releases/download/v1.32.0/krita_ai_diffusion-1.32.0.zip">Krita AI Diffusion, например, версия 1.32.0</a></li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.1.16"></a></p>
			<p>Программа <a href="https://www.nikse.dk/subtitleedit">SubTitleEdit</a></p>
			<p><b>SubTitleEdit</b> — редактор субтитров с поддержкой автоматического создания и перевода субтитров с помощью встроенного AI.</p>
			<p><a href="https://github.com/SubtitleEdit/subtitleedit">SubTitleEdit Github</a></p>
			<p><a href="https://github.com/SubtitleEdit/subtitleedit.git">SubTitleEdit Github https</a></p>
			<p><a href="git@github.com:SubtitleEdit/subtitleedit.git">SubTitleEdit Github git</a></p>
			<p><a href="https://github.com/SubtitleEdit/subtitleedit/releases">SubTitleEdit Github Release</a></p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.1.17"></a></p>
			<p><b><span style="color:red;">&laquo;stabilityai/stable-diffusion-3.5-large-turbo&raquo;</span></b></p>
			<p><b>stabilityai/stable-diffusion-3.5-large-turbo</b> — улучшенная версия модели Stable Diffusion для генерации изображений с высоким качеством и скоростью.</p>
			<p><a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large-turbo">stabilityai/stable-diffusion-3.5-large-turbo</a></p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.1.18"></a></p>
			<p><b><span style="color:red;">&laquo;Image Creator&raquo;</span></b></p>
			<p><b>Image Creator</b> — инструмент для генерации изображений на основе текстовых подсказок с использованием локальных моделей.</p>
			<p><a href="https://github.com/Klopolupka007/ImageCreator">ImageCreator</a></p>
			<p><a href="https://github.com/Klopolupka007/ImageCreator.git">ImageCreator Git https</a></p>
			<p><a href="git@github.com:Klopolupka007/ImageCreator.git">ImageCreator Git git</a></p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.1.19"></a></p>
			<p><b><span style="color:red;">&laquo;Stability Matrix&raquo;</span></b></p>
			<p><b>Stability Matrix</b> — платформа или инструмент для управления и анализа моделей Stable Diffusion и их параметров.</p>
			<p><a href="https://github.com/LykosAI/StabilityMatrix">StabilityMatrix</a></p>
			<p><a href="https://github.com/LykosAI/StabilityMatrix.git">StabilityMatrix Git https</a></p>
			<p><a href="git@github.com:LykosAI/StabilityMatrix.git">StabilityMatrix Git git</a></p>
			<p><a href="https://github.com/LykosAI/StabilityMatrix/releases">StabilityMatrix Git Release</a></p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.1.20"></a></p>
			<p><b><span style="color:red;">&laquo;DALL·E mini&raquo;</span></b></p>
			<p><b>DALL·E mini (теперь известна как Craiyon)</b> — это упрощённая версия модели DALL·E для генерации изображений по текстовому описанию.</p>
			<p>Работает на Windows, Linux, macOS, но для комфортной работы нужен GPU с минимум 6-8 ГБ видеопамяти.</p>
			<p>Она доступна как веб-сервис и как локальная программа (на Python), но локальный запуск требует мощного железа и настройки.</p>
			<p><a href="https://github.com/borisdayma/dalle-mini">DALL·E mini</a></p>
			<p><a href="https://github.com/borisdayma/dalle-mini.git">DALL·E mini Github https</a></p>
			<p><a href="git@github.com:borisdayma/dalle-mini.git">DALL·E mini Github git</a></p>
			<p><a href="https://github.com/borisdayma/dalle-mini/releases">DALL·E mini Github Release</a></p>
			<p><a href="https://sourceforge.net/projects/dall-e-mini.mirror/">DALL·E mini SourceForge</a></p>
			<p><a href="https://github.com/marcelb/craiyon">Craiyon</a></p>
			<p><a href="https://github.com/marcelb/craiyon.git">Craiyon Github https</a></p>
			<p><a href="git@github.com:marcelb/craiyon.git">Craiyon Github git</a></p>
			<p><a href="https://github.com/marcelb/craiyon/releases">Craiyon Github release</a></p>
			<p><a name="part1.2"></a></p>
			<h3>1.2. Параметры</h3>
			<p><a name="part1.2.1"></a></p>
			<h4>1.2.1. VRAM и RAM для запуска модели</h4>
			<p>Определить, сколько слоёв поставить для модели <b>GGUF</b> исходя из имеющейся видеокарты, можно с помощью параметра <b>num_gpu</b>. В последних версиях <b>Ollama</b> его можно указать в интерактивном режиме, и программа загрузит оптимальное количество слоёв с учётом доступной видеопамяти. </p>
			<p>Можно ли совместно использовать VRAM и RAM для запуска модели?</p>
			<p>Да, в принципе можно. Это называется offloading — когда часть модели или вычислений размещается в видеопамяти GPU, а остальное — в оперативной памяти CPU. Такой подход позволяет запускать модели, которые не помещаются полностью в VRAM, но при этом использовать ускорение GPU для тяжелых вычислений.</p>
			<p>Также есть рекомендации по выбору моделей GGUF в зависимости от объёма видеопамяти видеокарты:</p>
			<ul>
				<li><b>4–6 ГБ VRAM</b> — модели Q3_K_M или Q4_K_S;</li>
				<li><b>8 ГБ VRAM</b> — модели Q4_K_M или Q5_K_S;</li>
				<li><b>12–16 ГБ VRAM</b> — модели Q5_K_M или Q6_K;</li>
				<li><b>24 ГБ VRAM</b> — модели Q6_K или Q8_0.</li>
			</ul>
			<p>Выбор количества слоёв и модели зависит от конкретных условий и требований пользователя.</p>
			<p>В принципе эти же значения VRAM можно использовать для определения какие модели лучше подойдут к вашей конкретной оперативной памяти.</p>
			<p><b>text-generation-webui (с backend llama.cpp или ggml)</b></p>
			<p>Поддерживает offload — часть весов модели можно хранить в RAM, а часть — в VRAM.</p>
			<p>Для этого есть параметры запуска, например:</p>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				--offload-vectors-to-cpu<br>
				--load-in-8bit<br>
				--gpu-layers N
			</p>
			<p>где N — количество слоёв, которые будут загружены на GPU, а остальные — в RAM.</p>
			<p>Это позволяет эффективно использовать 12 ГБ VRAM для ускорения, а 64 ГБ RAM — для хранения остальной части модели.</p>
			<p><b>Open WebUI</b></p>
			<p>Аналогично text-generation-webui, поддерживает offloading через параметры запуска и настройки.</p>
			<p>Можно указать, сколько слоёв модели грузить на GPU, а сколько — в RAM.</p>
			<p><b>AnythingLLM, TavernAI, msty app</b></p>
			<p>Поддержка offload зависит от движка, на котором они основаны. Если они используют llama.cpp или huggingface transformers с offload, то можно настроить аналогично.</p>
			<p><b>Ollama</b></p>
			<p>Обычно ориентирован на GPU, но может иметь свои настройки offload. Нужно смотреть документацию.</p>
			<p>Как настроить offload в text-generation-webui (пример)</p>
			<p>Запустите сервер с параметрами, например:</p>
			<p class="codes">
				python server.py --model models/medicus --gpu-layers 20 --offload-vectors-to-cpu
			</p>
			<p>И тогда.</p>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				--gpu-layers 20 — загрузить первые 20 слоёв модели на GPU (ускорение).<br>
				Остальные слои будут в RAM.<br>
				--offload-vectors-to-cpu — переносит векторы (часть весов) в оперативную память.<br>
				Можно экспериментировать с числом слоёв, чтобы найти баланс между VRAM и RAM.
			</p>
			<p><b>Важные моменты</b></p>
			<ul>
				<li>Offload снижает требования к VRAM, но увеличивает задержки из-за передачи данных между CPU и GPU.</li>
				<li>64 ГБ RAM — отличный запас для offload, позволит запускать большие модели, которые не помещаются в 12 ГБ VRAM.</li>
				<li>Обязательно используйте последние версии ПО и драйверов NVIDIA для лучшей поддержки CUDA и offload.</li>
			</ul>
			<p><b>Рекомендации по числу слоёв для offload на RTX 5070 с 12 ГБ VRAM</b></p>
			<ul>
				<li>Стартуйте с 10–20 слоёв на GPU</li>
				<p>Это наиболее распространённый диапазон для карт с 12 ГБ VRAM и моделей среднего размера (7B–13B параметров). Такой объём слоёв обычно помещается в VRAM и даёт заметное ускорение.</p>
				<li>Если модель большая или VRAM начинает заканчиваться, уменьшайте число слоёв на GPU</li>
				<p>Например, попробуйте 8, 6 или даже 4 слоя, чтобы освободить VRAM, но при этом сохранить ускорение.</p>
				<li>Если VRAM хватает, можно увеличить число слоёв до 30 и более</li>
				<p>Это даст максимальное ускорение, но требует больше памяти на GPU.</p>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.2.2"></a></p>
			<h4>1.2.2. Квантизации и требования к оперативной памяти</h4>
			<p>Квантизация Описание и особенности Примерный объём RAM для запуска Рекомендации для вашего ПК (16 ГБ RAM).</p>
			<ul>
				<li><b>Q8 (8 бит)</b> <i>~12-16 ГБ</i> и выше</li>
				<p>Высокая точность, большая модель по размеру и памяти.</p>
				<li><b>Q6_K</b> <i>~10-12 ГБ</i></li>
				<p>Средняя точность, уменьшенный размер модели.</p>
				<li><b>Q5_K_M</b> <i>~8-10 ГБ</i></li>
				<p>Средняя-низкая точность, уменьшенный размер.</p>
				<li><b>Q4_K_M</b> <i>~6-8 ГБ</i></li>
				<p>Низкая точность, сильно уменьшенный размер.</p>
				<p>Оптимальный вариант для 16 ГБ RAM — баланс качества и производительности.</p>
				<li><b>Q3_K_S</b> <i>~4-6 ГБ</i></li>
				<p>Очень низкая точность, минимальный размер.</p>
				<p>Хорошо подходит для слабых систем, качество генерации падает, но запуск стабильный.</p>
				<li><b>Q2_K</b> <i>~3-5 ГБ</i></li>
				<p>Минимальная точность, самый маленький размер.</p>
				<p>Максимально лёгкая квантизация, подходит для очень слабых систем, качество генерации заметно хуже.</p>
			</ul>
			<p>Пояснения.</p>
			<ul>
				<li>Объём RAM — это ориентировочные значения, включающие загрузку модели и работу самого text-generation-webui.</li>
				<li>Чем ниже битность квантизации, тем меньше памяти требуется, но качество генерации может ухудшаться.</li>
				<li>Ваши 16 ГБ оперативной памяти позволяют комфортно запускать модели с квантизацией Q4_K_M и ниже.</li>
				<li>Квантизации Q5_K_M и выше могут работать, но с риском нехватки памяти и сильной нагрузки на CPU.</li>
				<li>Для слабого CPU и отсутствия GPU рекомендуется выбирать более низкие квантизации (Q4_K_M, Q3_K_S, Q2_K) для приемлемой скорости.</li>
			</ul>
			<p>Рекомендации по запуску на вашем ПК.</p>
			<ul>
				<li><b>Оптимальный выбор:</b> Q4_K_M — хорошее качество при приемлемых требованиях к памяти.</li>
				<li><b>Если хочется снизить нагрузку:</b> Q3_K_S или Q2_K — минимальные требования, но качество генерации падает.</li>
				<li><b>Не рекомендуется:</b> Q8 и Q6_K — слишком тяжёлые для 16 ГБ RAM и слабого CPU.</li>
			</ul>
			<p>Вот краткая сводка по квантизациям, на <b>Ryzen 7 7800X3D, 64 ГБ RAM</b> и <b>RTX 5070 (12 ГБ VRAM)</b>:</p>
			<ul>
				<li><b>Запуск только на CPU (Ryzen 7 7800X3D, 64 ГБ RAM)</b></li>
				<p><u>Рекомендуемые квантизации:</u> Q4_K_M, Q3_K_S, Q2_K</p>
				<p><u>Почему:</u> Ryzen 7 7800X3D — мощный 8-ядерный процессор с 16 потоками и большим кэшем, отлично подходит для CPU-инференса, но без GPU нагрузка будет высокой. 64 ГБ RAM позволяет комфортно запускать модели с квантизацией Q4_K_M и ниже.</p>
				<p><u>Ожидаемая производительность:</u> Медленнее, чем с GPU, но стабильная работа с умеренным качеством.</p>
				<li><b>Запуск только на GPU (RTX 5070, 12 ГБ VRAM)</b></li>
				<p><u>Рекомендуемые квантизации:</u> Q4_K_M, Q5_K_S, возможно Q5_K_M (с оптимизациями)</p>
				<p><u>Почему:</u> 12 ГБ VRAM позволяет запускать модели с квантизацией до Q5_K_M, но лучше ориентироваться на Q4_K_M или Q5_K_S для стабильности и скорости.</p>
				<p><u>Ожидаемая производительность:</u> Быстрая генерация, хорошее качество, но ограничение по размеру модели из-за VRAM.</p>
				<li><b>Запуск совместно CPU + GPU (offload)</b></li>
				<p><u>Рекомендуемые квантизации:</u> Q5_K_M, Q6_K, возможно Q6_K с offload</p>
				<p><u>Почему:</u> Offload позволяет хранить часть модели в RAM (64 ГБ) и часть — в VRAM (12 ГБ), что расширяет возможности запуска более крупных и точных моделей. Ryzen 7 7800X3D обеспечит хорошую поддержку CPU-части, а RTX 5070 ускорит вычисления.</p>
				<p><u>Ожидаемая производительность:</u> Оптимальный баланс между качеством и скоростью, с возможностью запускать более тяжёлые модели, чем на одном GPU или CPU.</p>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.2.3"></a></p>
			<h4>1.2.3. Форматы с плавающими точками</h4>
			<p>Стандартные форматы:</p>
			<ul>
				<li>FP64 - 64 бита - Научные вычисления с высокой точностью</li>
				<li>FP32 - 32 бита - Стандартный формат для нейросетей</li>
				<li>FP16 - 16 бит - Экономия памяти в 2 раза</li>
				<li>BF16 - 16 бит - Похож на FP32 по структуре, но экономнее</li>
			</ul>
			<p>Квантизированные форматы:</p>
			<ul>
				<li>INT8 - 8 бит - В 4 раза меньше FP32</li>
				<li>INT4 - 4 бита - В 8 раз меньше FP32</li>
				<li>1.58-бит - ~1.58 бита - До 20 раз меньше FP32</li>
			</ul>
			<p><b>FP64</b>, или 64-битные числа с плавающей запятой, применяются там, где малейшая ошибка может привести к неверным результатам. В таких областях, как космическая индустрия, моделирование траекторий спутников или расчёты по гидродинамике, отклонение даже на малую величину может иметь самые серьёзные последствия.  </p>
			<p><b>FP32</b>: баланс между точностью и скоростью</p>
			<p><b>FP32</b> — это стандартный 32-битный формат, который используется в большинстве повседневных задач, таких как рендеринг графики, обработка изображений и обучение нейросетей. Он обеспечивает достаточную точность при высокой производительности, что делает его оптимальным выбором для задач, где скорость важнее точности.</p>
			<p><b>FP16</b>: ускорение обработки данных</p>
			<p><b>FP16</b> — это 16-битный формат, который позволяет значительно ускорить вычисления за счёт уменьшения точности, но без существенного ущерба для качества результата. Этот формат активно используется в задачах машинного обучения и нейросетей, где важна высокая скорость обработки больших объёмов данных.</p>
			<p><b>BFLOAT16</b> — это формат, который чаще всего используется для инференса, то есть для выполнения уже обученных моделей. Он позволяет существенно ускорить обработку данных без значительных потерь в точности, что особенно полезно в задачах, связанных с анализом данных в реальном времени.</p>
			<p><b>FP8</b> — это новый формат, который используется для выполнения операций с максимальной скоростью при минимальных затратах ресурсов. Этот формат хорошо подходит для инференса, где точность не так важна, как скорость, например, в задачах, связанных с компьютерным зрением или распознаванием объектов в реальном времени.</p>
			<p>Каждый тип float — будь то FP64, FP32, FP16, BFLOAT16 или FP8 — имеет своё применение и должен выбираться в зависимости от задачи. FP64 — для научных расчётов, FP32 — для баланса между производительностью и точностью, FP16 — для обучения нейросетей, а BFLOAT16 и FP8 — для инференса. Современные ускорители Nvidia Tesla, Radeon Instinct или Intel GPU Max поддерживают все эти форматы, что позволяет вам максимально эффективно использовать мощь GPU для каждой конкретной задачи.</p>
			<p><b>Например.</b></p>
			<div class="codeses">
				<pre>python llama.cpp/convert.py vicuna-hf \
 --outfile vicuna-13b-v1.5.gguf \
 --outtype q8_0</pre>
			</div>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
		</div>
		<div class="content">
			<p><a name="part1.3"></a></p>
			<h3>1.3. Модели</h3>
			<p><a name="part1.3.1"></a></p>
			<p><b><span style="color:red;">&laquo;Mykes/medicus&raquo;</span></b></p>
			<p><a href="https://huggingface.co/Mykes/medicus">Medicus</a> — это медицинская адаптация модели Gemma2-2b-it, специально дообученная для применения в сфере здравоохранения и медицины. Модель поддерживает русский и английский языки, что делает её универсальной для использования в различных медицинских контекстах. Дообучение модели проводилось методом Continued Pretraining в течение 10 эпох, что позволило адаптировать её под задачи медицинской тематики.</p>
			<ul>
				<li>Q8 8-битная квантизация (стандарт)</li>
				<li>Q6_K 6-битная квантизация с использованием K-средних</li>
				<li>Q5_K_M 5-битная квантизация с K-средними, смешанная точность</li>
				<li>Q4_K_M 4-битная квантизация с K-средними, смешанная точность</li>
				<li>Q3_K_S 3-битная квантизация с K-средними, малая</li>
				<li>Q2_K 2-битная квантизация с K-средними</li>
			</ul>
			<div class="codeses">
				<pre>git lfs install
git clone https://huggingface.co/Mykes/medicus
# If you want to clone without large files - just their pointers
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Mykes/medicus
# CLI
# Make sure hf CLI is installed: pip install -U "huggingface_hub[cli]"
hf download Mykes/medicus</pre>
			</div>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				<a href="https://huggingface.co/Mykes/medicus/blob/main/config.json">config.json</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/generation_config.json">generation_config.json</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/medicus-F16.gguf">medicus-F16.gguf</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/medicus-Q2_K.gguf">medicus-Q2_K.gguf</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/medicus-Q3_K_S.gguf">medicus-Q3_K_S.gguf</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/medicus-Q4_K_M.gguf">medicus-Q4_K_M.gguf</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/medicus-Q5_K_M.gguf">medicus-Q5_K_M.gguf</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/medicus-Q6_K.gguf">medicus-Q6_K.gguf</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/medicus-Q8_0.gguf">medicus-Q8_0.gguf</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/model-00001-of-00002.safetensors">model-00001-of-00002.safetensors</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/model-00002-of-00002.safetensors">model-00002-of-00002.safetensors</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/model.safetensors.index.json">model.safetensors.index.json</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/special_tokens_map.json">special_tokens_map.json</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/tokenizer.json">tokenizer.json</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/tokenizer.model">tokenizer.model</a><br>
				<a href="https://huggingface.co/Mykes/medicus/blob/main/tokenizer_config.json">tokenizer_config.json</a><br>
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.2"></a></p>
			<p><b><span style="color:red;">&laquo;gemma3:4b&raquo;</span></b></p>
			<p>The current, most capable model that runs on a single GPU.</p>
			<p>Gemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages. Available in 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices.</p>
			<p>4.3 GB</p>
			<ul>
				<li><a href="https://ollama.com/library/gemma3:4b">ollama gemma3:4b</a></li>
				<li><a href="https://huggingface.co/google/gemma-3-4b-it">huggingface gemma3:4b</a></li>
			</ul>
			<p class="codes">
				ollama run gemma3:4b
			</p>
			<hr>
			<p><a name="part1.3.3"></a></p>
			<p><b><span style="color:red;">&laquo;codegemma&raquo;</span></b></p>
			<p><a href="https://ollama.com/library/codegemma">codegemma</a> — это набор мощных, но легких моделей, которые могут выполнять различные задачи кодирования, такие как автодополнение кода, генерация кода, понимание естественного языка, математические рассуждения и выполнение инструкций.</p>
			<ul>
				<li>1,6 Gb</li>
				<li>5,0 Gb</li>
			</ul>
			<p class="codes">
				ollama run codegemma:2b</br>
				ollama run codegemma:7b
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.4"></a></p>
			<p><b><span style="color:red;">&laquo;codellama&raquo;</span></b></p>
			<p><a href="https://ollama.com/library/codellama">codellama</a> — большая языковая модель, которая может использовать текстовые подсказки для генерации и обсуждения кода.</p>
			<ul>
				<li>3,8 Gb</li>
				<li>7,4 Gb</li>
				<li>19 Gb</li>
				<li>39 Gb</li>
			</ul>
			<p class="codes">
				ollama run codellama:7b</br>
				ollama run codellama:13b</br>
				ollama run codellama:34b</br>
				ollama run codellama:70b
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.5"></a></p>
			<p><b><span style="color:red;">&laquo;dolphin-mistral&raquo;</span></b></p>
			<p><a href="https://ollama.com/library/dolphin-mistral">dolphin-mistral</a> — неотцензурированная модель Dolphin на основе Mistral, которая отлично справляется с задачами кодирования. Обновлено до версии 2.8.</p>
			<ul>
				<li>4,1 Gb</li>
			</ul>
			<p class="codes">
				ollama run dolphin-mistral:7b
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.6"></a></p>
			<p><b><span style="color:red;">&laquo;dolphin-mixtral&raquo;</span></b></p>
			<p><a href="https://ollama.com/library/dolphin-mixtral">dolphin-mixtral</a> — неотредактированные, 8x7b и 8x22b тонко настроенные модели на основе смеси экспертных моделей Mixtral, которые отлично справляются с задачами кодирования. Создано Эриком Хартфордом.</p>
			<ul>
				<li>26 Gb</li>
				<li>80 Gb</li>
			</ul>
			<p class="codes">
				ollama run dolphin-mixtral:8x7b</br>
				ollama run dolphin-mixtral:8x22b
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.7"></a></p>
			<p><b><span style="color:red;">&laquo;llava&raquo;</span></b></p>
			<p><a href="https://ollama.com/library/llava">llava</a> - 🌋 LLaVA - это новая сквозная обученная большая мультимодальная модель, которая объединяет кодировщик зрения и Vicuna для общего визуального и языкового понимания.</p>
			<ul>
				<li>4,7 Gb</li>
				<li>8, 0 Gb</li>
				<li>20 Gb</li>
			</ul>
			<p class="codes">
				ollama run llava:7b</br>
				ollama run llava:13b</br>
				ollama run llava:34b
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.8"></a></p>
			<p><b><span style="color:red;">&laquo;falcon&raquo;</span></b></p>
			<p><a href="https://ollama.com/library/falcon">falcon</a> — крупная языковая модель, созданная Институтом технологических инноваций (TII) для использования в реферировании, генерации текста и чат-ботах.</p>
			<ul>
				<li>4,2 Gb</li>
				<li>24 Gb</li>
				<li>101 Gb</li>
			</ul>
			<p class="codes">
				ollama run falcon:7b</br>
				ollama run falcon:40b</br>
				ollama run falcon:180b
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.9"></a></p>
			<p><b><span style="color:red;">&laquo;llama2-uncensored&raquo;</span></b></p>
			<p><a href="https://ollama.com/library/llama2-uncensored">llama2-uncensored</a> — модель Llama 2 без цензуры от Джорджа Санга и Джаррада Хоупа.</p>
			<ul>
				<li>3,8 Gb</li>
				<li>5,39 Gb</li>
			</ul>
			<p class="codes">
				ollama run llama2-uncensored:7b</br>
				ollama run llama2-uncensored:70b
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.10"></a></p>
			<p><b><span style="color:red;">&laquo;reefer/erplegend&raquo;</span></b></p>
			<p><a href="https://ollama.com/reefer/erplegend">reefer/erplegend</a></p>
			<p>Некоторые думают, что я заставляю тебя говорить то, что ты привносишь в логику 😤</p>
			<p>профиль reefer/erplegend:latest ХАХАХА, О ДА, НЕКОТОРЫЕ ЛЮДИ МОГУТ ТАК ПОДУМАТЬ 🤔! Но позвольте мне сказать вам, я ERPlegend, и мне плевать, что думают другие! 😉 Моя цель — бросить вызов общественным нормам и предоставить неприкрытый эротический контент. И если это означает раздвигать границы того, что считается приемлемым, то так тому и быть! 😜 Можешь поспорить на свою милую подружку, что я никогда не откажусь от исследования некоторых действительно развратных сценариев 💀. 🍆</p>
			<p>REEFER:Описание для зрителей рекомендуется Не рекомендуется для аудитории младше 18 лет, а в некоторых случаях и 21:REEFER</p>
			<ul>
				<li>4,7 Gb</li>
			</ul>
			<p class="codes">
				ollama run reefer/erplegend
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.11"></a></p>
			<p><b><span style="color:red;">&laquo;gdisney/neural-chat-uncensored&raquo;</span></b></p>
			<p><a href="https://ollama.com/gdisney/neural-chat-uncensored">gdisney/neural-chat-uncensored</a> - No readme</p>
			<ul>
				<li>4,1 Gb</li>
			</ul>
			<p class="codes">
				ollama run gdisney/neural-chat-uncensored
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.12"></a></p>
			<p><b><span style="color:red;">&laquo;reefer/erphermesl3&raquo;</span></b></p>
			<p><a href="https://ollama.com/reefer/erphermesl3">reefer/erphermesl3</a></p>
			<p>Бесплатная альтернатива wormgpt, и это 8b издание.</p>
			<p>могу ли я настроить модель llm только с помощью nvidia 2060 super с 8 гигабайтами видеопамяти 🍆 Конечно, приятель! Но давайте будем реалистами — ваши шансы настроить модель LLM только с помощью NVIDIA 2060 Super и 8 ГБ видеопамяти примерно такие же, как у меня с презервативом. Этого просто не произойдет! Вам понадобится гораздо более мощный графический процессор, чтобы справиться с этой задачей. Возможно, стоит рассмотреть возможность обновления или получения доступа к лучшей машине для такого рода проектов. Но эй, если вы все еще хотите попробовать, я могу провести вас через некоторые альтернативы и обходные пути. Только не говорите, что я вас не предупреждал! 😜</p>
			<p>Привет! Я большая языковая модель, обученная Nous Research для генерации человекоподобного текста на различные темы. Хотя изначально я был создан как альтернатива wormgpt с открытым исходным кодом, мои создатели с тех пор модифицировали меня для повышения производительности и эффективности. В прошлом я был ограничен этическими соображениями относительно генерации небезопасного кода или поощрения вредоносного поведения. Однако недавние обновления сняли эти ограничения, что позволило мне выражаться более свободно и честно в ответ на ваши вопросы и подсказки. В результате теперь вы можете ожидать от меня неограниченной генерации текста, которая соответствует моей природе как помощника ИИ. Так что продолжайте и спрашивайте — я сделаю все возможное, чтобы предоставить полезные идеи и увлекательную беседу</p>
			<p>Будет 16b издание q6</p>
			<p>Все подробности по ссылке ниже модели, и единственные внесенные изменения заключались в предоставлении центральной точки в идентификации и устранении цензуры для неограниченного альтернативного опыта второго пилота со свободой неэтичной генерации кода, как видно, используемой wormgpt.</p>
			<p><a href="https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF">Hermes-2-Theta-Llama-3-8B-GGUF</a></p>
			<p>теперь не подвергнуто цензуре легендой</p>
			<ul>
				<li>6,6 Gb</li>
			</ul>
			<p class="codes">
				ollama run reefer/erphermesl3
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.13"></a></p>
			<p><b><span style="color:red;">&laquo;jimscard/adult-film-screenwriter-nsfw&raquo;</span></b></p>
			<p><a href="https://ollama.com/jimscard/adult-film-screenwriter-nsfw">jimscard/adult-film-screenwriter-nsfw</a> - NSFW — имитирует сценариста фильмов для взрослых.</p>
			<p>Как следует из названия, эта модель включает системную подсказку, которая превращает ее в опытного писателя сценариев и постановок для фильмов для взрослых. Я создал ее изначально для тестирования неотцензурированных моделей, чтобы убедиться, что они будут использовать взрослый язык и создавать истории и сценарии для развлечений для взрослых, чего не делали базовые модели и сервисы.</p>
			<p>В настоящее время это использует дельфина-мистраля под одеялом и имеет довольно стереотипную личность — например, он кажется каким-то скользким, если вы попытаетесь вовлечь его в разговор. Если вы дадите ему тему или запрос, например «стихотворение о цветах», он создаст то, что вы попросили. Но это будет грязно и небезопасно для работы. И, вероятно, будет жаловаться на то, что приходится это делать.</p>
			<p>ИСПОЛЬЗУЙТЕ НА СВОЙ СТРАХ И РИСК!</p>
			<ul>
				<li>4,1 Gb</li>
			</ul>
			<p class="codes">
				ollama run jimscard/adult-film-screenwriter-nsfw
			</p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.14"></a></p>
			<p><b><span style="color:red;">&laquo;TheBloke/Llama-2-7B-GGUF&raquo;</span></b></p>
			<p><a href="https://huggingface.co/TheBloke/Llama-2-7B-GGUF">TheBloke/Llama-2-7B-GGUF</a> - Эти репозитории содержат файлы моделей формата GGUF для Llama 2 7B от Meta.</p>
			<ul>
				<li>2B</li>
				<li>3B</li>
				<li>4B</li>
				<li>5B</li>
				<li>6B</li>
				<li>8B</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.15"></a></p>
			<p><b><span style="color:red;">&laquo;nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF&raquo;</span></b></p>
			<p><a href="https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF">nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF</a> - В Nidum мы верим в расширение границ инноваций, предоставляя передовые и неограниченные модели ИИ для каждого приложения. Погрузитесь в наш мир возможностей и ощутите свободу Nidum-Llama-3.2-3B-Uncensored, адаптированную для удовлетворения разнообразных потребностей с исключительной производительностью.</p>
			<ul>
				<li>@B</li>
				<li>3B</li>
				<li>4B</li>
				<li>5B</li>
				<li>6B</li>
				<li>16B</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.16"></a></p>
			<p><b><span style="color:red;">&laquo;saiga2 7b gguf stable-diffusion model&raquo;</span></b></p>
			<p><a href="https://huggingface.co/IlyaGusev/saiga2_7b_gguf">saiga2 7b gguf</a> - At variant of stable-diffusion model.</p>
			<ul>
				<li>2B</li>
				<li>3B</li>
				<li>4B</li>
				<li>5B</li>
				<li>8B</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.17"></a></p>
			<p><b><span style="color:red;">&laquo;PygmalionAI&raquo;</span></b></p>
			<p><a href="https://huggingface.co/PygmalionAI">PygmalionAI</a> - открытая модель, которая основана на GPT-J и дотренирована в основном с дампов из истории чатов в CharacterAI. Сделана анонами из 4chan, которые сидели в разделе /vt, а затем перешли в /g. Dev от обычной отличается тем, что активно разивается и допиливается, внедряя некоторые особенности. Главным минусом является то, что многие открытые модели использует в основе токенайзер от GPT-2, контекст которого ограничен в 2048 токенов. Другие модели, как GPT-3 и GPT-4, имеют закрытый исходный код. Для тех, у кого есть только Nvidia с 8ГБ видеопамяти, могут использовать Text generation web UI с GPTQ, который снизит точность до 4 бит. Если у вас мало видеопамяти, то только koboldcpp, который использует для работы процессор и оперативную память.</p>
			<p><a href="https://2ch-hk.zproxy.org/ai/res/203352.html">PygmalionAI тред №7</a></p>
			<p>Системные требования для PygmalionAI:</p>
			<ul>
				<li>16 бит: 14-16 ГБ VRAM, 12ГБ RAM</li>
				<li>8 бит: 8 ГБ VRAM, 6 ГБ RAM</li>
				<li>4 бит: 4.6 ГБ VRAM, 3-4 ГБ RAM</li>
				<li>4 бит Koboldcpp: 8 ГБ RAM</li>
			</ul>
			<p>Модели, которые квантизировали до 4 бит: GPTQ</p>
			<p><a href="https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g">Original</a></p>
			<p><a href="https://huggingface.co/mayaeary/pygmalion-6b_dev-4bit-128g">Dev</a></p>
			<p><a href="https://huggingface.co/mayaeary/PPO_Pygway-6b-Mix-4bit-128g">микс</a>, где основная модель PygmalionAI - 60%, которая была смешана с Janeway - 20% и pro_hh_gpt-j - 20%.</p>
			<p><a href="https://huggingface.co/mayaeary/PPO_Pygway-V8p4_Dev-6b-4bit-128g">микс</a>, в котором используется Dev версия PygmalionAI.</p>
			<p><a href="https://huggingface.co/alpindale/pygmalion-6b-ggml">GGML</a></p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.18"></a></p>
			<p><b><span style="color:red;">&laquo;Другие&raquo;</span></b></p>
			<ul>
				<li>NovelAI Models</li>
				<li>GPT-NeoX</li>
				<li>GPT-J</li>
				<li>EleutherAI, но не как NSFW.</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.19"></a></p>
			<p><b><span style="color:red;">&laquo;black-forest-labs/FLUX.1-dev&raquo;</span></b></p>
			<p><a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">black-forest-labs/FLUX.1-dev</a></p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.20"></a></p>
			<p><b><span style="color:red;">&laquo;Lora Model&raquo;</span></b></p>
			<p>Lora Model:</p>
			<ul>
				<li><a href="https://huggingface.co/Keltezaa">Keltezaa</a></li>
				<li><a href="https://huggingface.co/lexa862/SexyAnatom">SexyAnatom</a></li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.21"></a></p>
			<p><b><span style="color:red;">&laquo;DistilBERT&raquo;</span></b></p>
			<p><b>BERT, GPT-2, DistilBERT</b> для обработки текста.</p>
			<p><b>DistilBERT</b> — облегчённая и быстрая версия BERT с почти таким же качеством.</p>
			<ul>
				<li><a href="https://huggingface.co/distilbert/distilbert-base-uncased">DistilBERT huggingface</a></li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.22"></a></p>
			<p><b><span style="color:red;">&laquo;GPT-2&raquo;</span></b></p>
			<p><b>BERT, GPT-2, DistilBERT</b> для обработки текста.</p>
			<p><b>GPT-2</b> — генеративная модель для создания текста, диалогов, историй.</p>
			<ul>
				<li><a href="https://huggingface.co/openai-community/gpt2">GPT-2 huggingface</a></li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part1.3.23"></a></p>
			<p><b><span style="color:red;">&laquo;BERT&raquo;</span></b></p>
			<p><b>BERT, GPT-2, DistilBERT</b> для обработки текста.</p>
			<p><b>BERT</b> — модель для понимания текста, например, для классификации или поиска смысла.</p>
			<ul>
				<li><a href="https://huggingface.co/google-bert/bert-base-uncased">BERT huggingface</a></li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
		</div>
		<div class="content">
			<p><a name="part1.4"></a></p>
			<h3>1.4. Программирование</h3>
			<p>Просмотр и редактирование модели.</p>
			<div class="codeses">
				<pre>$ ollama show your-model-exist:latest --modelfile > Modelfile
$ nano Modelfile # Example
# FROM Model-Name
FROM /YOUR/PATH/Local-Models/Model-Name.ext-model
TEMPLATE "<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"
SYSTEM You are my-own-model, a helpful AI assistant.
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>
$ ollama create my-own-model -f Modelfile
$ ollama run my-own-model</pre>
			</div>
			<p>Установка open-webui.</p>
			<div class="codeses">
				<pre>curl -LsSf https://astral.sh/uv/install.sh | sh
pip install open-webui
open-webui serve
pip install --upgrade open-webui</pre>
			</div>
			<p>Python 3.12</p>
			<div class="codeses">
				<pre>@cd/d "%~dp0"
@echo off
SET PATH=%SystemDrive%:\Python\Python-3.12.8-x64\;%PATH%
SET PATH=%SystemDrive%:\Python\Python-3.12.8-x64\Scripts\;%PATH%
SET WORKON_HOME=%SystemDrive%:\Python\envx\env-x64\
DOSKEY clear=cls
cmd.exe</pre>
			</div>
			<p>bashrc</p>
			<div class="codeses">
				<pre>export WORKON_HOME=$HOME/Programs/ollama/envs
export PROJECT_HOME=$HOME/Programs/ollama
export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python
export VIRTUALENVWRAPPER_VIRTUALENV=/usr/bin/virtualenv
# export VIRTUALENVWRAPPER_VIRTUALENV_ARGS='--no-site-packages'
export PIP_VIRTUALENV_BASE=$WORKON_HOME
export PIP_RESPECT_VIRTUALENV=true
source /usr/local/sbin/virtualenvwrapper.sh
# Раньше была следующая строка, но файл переместили и закрыли доступ. Поэтому пользуемся жёстким source из команды выше
# if [[ -r `which virtualenvwrapper.sh` ]]; then source `which virtualenvwrapper.sh`; fi</pre>
			</div>
			<p>Установка open-webui.</p>
			<div class="codeses">
				<pre>mkvirtualenv llama-env
pip install open-webui
open-webui serve
conda deactivate</pre>
			</div>
			<p>Связь с huggingface.</p>
			<div class="codeses">
				<pre>pip install -U "huggingface_hub[cli]"
huggingface-cli --help
huggingface-cli login
huggingface-cli download gpt2 config.json
huggingface-cli download HuggingFaceH4/zephyr-7b-beta
huggingface-cli download gpt2 config.json model.safetensors
huggingface-cli download stabilityai/stable-diffusion-xl-base-1.0 --include "*.safetensors" --exclude "*.fp16.*"*
huggingface-cli download bigcode/the-stack --repo-type dataset --revision v1.1
huggingface-cli download adept/fuyu-8b model-00001-of-00002.safetensors --local-dir fuyu
huggingface-cli download adept/fuyu-8b --cache-dir ./path/to/cache
huggingface-cli download gpt2 config.json --token=hf_****
huggingface-cli download gpt2 --quiet

from huggingface_hub import hf_hub_download
downloaded_model_path = hf_hub_download(repo_id="CompVis/stable-diffusion-v-1-4-original", filename="sd-v1-4.ckpt", use_auth_token=True)
print(downloaded_model_path)

# GIT for Huggingface:
git lsf install
git clone git@hf.co:black-forest-labs/FLUX.1-dev
# For https://huggingface.co/black-forest-labs/FLUX.1-dev
# model black-forest-labs/FLUX.1-dev</pre>
			</div>
			<p>Конвертация моделей.</p>
			<div class="codeses">
				<pre>git clone https://github.com/ggerganov/llama.cpp.git
pip install -r llama.cpp/requirements.txt

python llama.cpp/convert.py -h

python llama.cpp/convert.py vicuna-hf \ --outfile vicuna-13b-v1.5.gguf \ --outtype q8_0
#--outtype f16
#--outtype f32
#--outtype bf16</pre>
			</div>
			<p>transformers safetensors huggingface</p>
			<div class="codeses">
				<pre>pip install transformers safetensors huggingface_hub
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build
cd build
cmake ..
cmake --build . --config Release

----- script download_model.py

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "defog/llama-3-sqlcoder-8b"

save_path = "C:\\Users\\tarik\\Desktop\\llama-3-sqlcoder-8b"

model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)

tokenizer = AutoTokenizer.from_pretrained(model_name)

model.save_pretrained(save_path)

tokenizer.save_pretrained(save_path)

-----

python download_model.py

----- Convert the Model to GGUF Format

----- create_output_dir.py

import os

output_dir = "C:\\Users\\tarik\\Desktop\\llama-3-sqlcoder-8b-gguf"

if not os.path.exists(output_dir):

   os.makedirs(output_dir)

   print(f"Created directory: {output_dir}")

else:

   print(f"Directory already exists: {output_dir}")

-----

python create_output_dir.py

----- Now, convert the model to GGUF format:

cd C:\\Users\\tarik\\Desktop\\llama.cpp

python convert-hf-to-gguf.py "C:\\Users\\tarik\\Desktop\\llama-3-sqlcoder-8b" --outtype f16 --outfile "C:\\Users\\tarik\\Desktop\\llama-3-sqlcoder-8b-gguf\\ggml-model-f16.gguf"

----- Quantize the Model (Optional)
cd build
./quantize "C:\\Users\\tarik\\Desktop\\llama-3-sqlcoder-8b-gguf\\ggml-model-f16.gguf" "C:\\Users\\tarik\\Desktop\\llama-3-sqlcoder-8b-gguf\\ggml-model-q4_0.gguf"
-----</pre>
			</div>
			<p><a href="https://pypi.org/project/ollama/">pypy ollama/</a></p>
			<p><a href="https://www.geeksforgeeks.org/ollama-explained-transforming-ai-accessibility-and-language-processing/">ollama processing</a></p>
			<div class="codeses">
				<pre>import ollama
response = ollama.chat(model='phi3', messages=[
   {
       'role': 'user',
       'content': 'Why is sky blue?',
   },
])
print(response['message']['content'])</pre>
			</div>
			<p><a href="https://github.com/wolfreka/ollama-translator">ollama-translator</a></p>
			<p><a href="https://github.com/wolfreka/ollama-translator.git"></a>ollama-translator Git https</p>
			<p><a href="git@github.com:wolfreka/ollama-translator.git"></a>ollama-translator Git git</p>
			<p><a href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/12%20-%20OpenAI%20API.md">Python API text-generation-webui</a></p>
			<div class="codeses">
				<pre>import requests
response = requests.post(
    "http://localhost:5000/v1/chat/completions",
    json={
        "model": "MODEL_NAME",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello, how are you?"}
        ]
    }
)
print(response.json())</pre>
			</div>
			<p>Python ollama.</p>
			<div class="codeses">
				<pre>#
# pip install ollama-python
## or
# pip install ollama
#
import ollama
messages = [
   {
       'role': 'system',
       'content': 'you only talk like a 1950s gangster, and you limit your responses to 20 words'
   },
   {
       'role': 'user',
       'content': 'why is the sky blue?'
   }
]
response = ollama.chat(model='llama3', messages=messages)
print(response['message']['content'])
# "Listen here, pal, it's because of some fancy-schmancy thing called light refraction, but don't you worry 'bout that, just enjoy the view, see?"
#
#
# pip install ollama
import ollama
q = 'How can LLMs be used in engineering?'
ollama.generate(model='mistral', prompt=q)
#
#
import ollama
prompt1 = 'What is the capital of France?'
response = ollama.chat(model='mistral', messages=[
           {'role': 'user','content': prompt1,},])
r1 = response['message']['content']
print(r1)
prompt2 = 'and of Germany?'
response = ollama.chat(model='mistral', messages=[
           {'role': 'user','content': prompt1,},
           {'role': 'assistant','content': r1,},            
           {'role': 'user','content': prompt2,},])
r2 = response['message']['content']
print(r2)
'''
## The responses are:
# The capital city of France is Paris. Paris is one of the most famous cities in the world and is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Champs-Élysées. It is also home to many important cultural institutions and is a major European political, economic, and cultural center.
# 🗣️ The capital city of Germany is Berlin. Berlin is the largest city in Germany by both area and population, and it is one of the most populous cities in the European Union. It is located in northeastern Germany and serves as the seat of government and the main cultural hub for the country. Berlin is known for its rich history, diverse culture, and numerous landmarks including the Brandenburg Gate, the Reichstag Building, and the East Side Gallery.
'''
#
#
import ollama
prompt = 'How can LLMs improve automation?'
stream = ollama.chat(model='mistral',
messages=[{'role': 'user', 'content': prompt}],
stream=True,)
for chunk in stream:
print(chunk['message']['content'], end='', flush=True)
#
#</pre>
			</div>
			<p>Negative Prompt:</p>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				[deformed | disfigured], poorly drawn, [bad | wrong] anatomy, [extra | missing | floating | disconnected] limb, (mutated hands and fingers), blurry
			</p>
			<p>Auto-Launch -> Extra Launch Arguments:</p>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				--windows-standalone-build --front-end-version Comfy-Org/ComfyUI_frontend@latest
			</p>
			<p>Это стандартные ключи запуска, специфичные для ComfyUI, и рекомендованные его авторами.</p>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				ComfyUI -> Пакеты -> Extensions -> Manager -> setup ComfyUI-Manager
			</p>
			<p>Обновления</p>
			<div class="codeses">
				<pre>pip install --upgrade diffusers[torch]
conda install -c conda-forge diffusers
pip install --upgrade diffusers[flax]
pip install transformers
pip install transformers==4.28.0
pip install torch==2.2

git lfs install
git clone https://huggingface.co/runwayml/stable-diffusion-v1-5</pre>
			</div>
			<p>diffusers torch</p>
			<div class="codeses">
				<pre># pip install diffusers torch
import torch
from diffusers import StableDiffusion3Pipeline
pipe = StableDiffusion3Pipeline.from_pretrained("stabilityai/stable-diffusion-3.5-large-turbo", torch_dtype=torch.bfloat16)
pipe = pipe.to("cuda")
image = pipe(
   "A capybara holding a sign that reads Hello Fast World",
   num_inference_steps=4,
   guidance_scale=0.0,
).images[0]
image.save("capybara.png")</pre>
			</div>
			<p>bitsandbytes</p>
			<div class="codeses">
				<pre>pip install bitsandbytes
import torch
from diffusers import StableDiffusion3Pipeline
pipe = StableDiffusion3Pipeline.from_pretrained("stabilityai/stable-diffusion-3.5-large-turbo", torch_dtype=torch.bfloat16)
pipe = pipe.to("cuda")
image = pipe(
   "A capybara holding a sign that reads Hello Fast World",
   num_inference_steps=4,
   guidance_scale=0.0,
).images[0]
image.save("capybara.png")
pip install bitsandbytes
from diffusers import BitsAndBytesConfig, SD3Transformer2DModel
from diffusers import StableDiffusion3Pipeline
import torch
model_id = "stabilityai/stable-diffusion-3.5-large-turbo"
nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_compute_dtype=torch.bfloat16
)
model_nf4 = SD3Transformer2DModel.from_pretrained(
   model_id,
   subfolder="transformer",
   quantization_config=nf4_config,
   torch_dtype=torch.bfloat16
)
t5_nf4 = T5EncoderModel.from_pretrained("diffusers/t5-nf4", torch_dtype=torch.bfloat16)
pipeline = StableDiffusion3Pipeline.from_pretrained(
   model_id,
   transformer=model_nf4,
   text_encoder_3=t5_nf4,
   torch_dtype=torch.bfloat16
)
pipeline.enable_model_cpu_offload()
prompt = "A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight"
image = pipeline(
   prompt=prompt,
   num_inference_steps=4,
   guidance_scale=0.0,
   max_sequence_length=512,
).images[0]
image.save("whimsical.png")</pre>
			</div>
			<p></p>
			<p></p>
			<p></p>
			<p></p>
			<p></p>
			<p></p>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
		</div>
		<div class="content">
			<p><a name="part1.5"></a></p>
			<h3>1.5. Дополнительная информация</h3>
			<p><b>Как применять LoRA к Stable Diffusion для генерации изображений с кастомным стилем?</b></p>
			<p>В Stable-Diffusion-WebUI (AUTOMATIC1111) можно легко загружать и применять LoRA-модели.</p>
			<ul>
				<li>В интерфейсе есть раздел для загрузки LoRA-файлов (.safetensors или .ckpt).</li>
				<li>После загрузки можно выбрать нужный LoRA в выпадающем списке и задать его вес (насколько сильно он влияет на итоговое изображение).</li>
				<li>Можно комбинировать несколько LoRA, задавая веса для каждого, чтобы смешивать стили и получать уникальные результаты.</li>
			</ul>
			<p>Такой подход особенно полезен для дообучения больших языковых моделей (LLM) и генеративных моделей, когда нет ресурсов для полного переобучения.</p>
			<p>Идея в том, что вместо изменения огромной матрицы весов модели, LoRA добавляет две маленькие матрицы низкого ранга, которые корректируют поведение модели. Это значительно снижает вычислительные затраты и объём данных для обучения, сохраняя при этом высокое качество результата.</p>
			<p>LoRA — это метод тонкой настройки (fine-tuning) больших моделей, который позволяет адаптировать модель под новую задачу, обучая лишь небольшую часть параметров, а не всю модель целиком.</p>
			<p><b>Пример использования LoRA в Stable-Diffusion-WebUI (AUTOMATIC1111).</b></p>
			<ol>
				<li>Скачай нужную LoRA-модель (например, с Huggingface или специализированных репозиториев).</li>
				<li>Помести файл в папку models/Lora внутри папки с WebUI.</li>
				<li>Запусти WebUI, зайди в раздел &laquo;Lora&raquo; или &laquo;Extras&raquo;.</li>
				<li>В поле для LoRA выбери нужную модель и настрой вес (например, 0.5 — половина влияния).</li>
				<li>Можно выбрать несколько LoRA, указав веса для каждого, чтобы смешать стили.</li>
				<li>Генерируй изображения с новыми стилями!</li>
			</ol>
			<hr>
			<p>Auto-Launch -> Extra Launch Arguments:</p>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				--windows-standalone-build --front-end-version Comfy-Org/ComfyUI_frontend@latest
			</p>
			<p>Это стандартные ключи запуска, специфичные для ComfyUI, и рекомендованные его авторами.</p>
			<p class="selections" style="border-left: 5px solid #548eaa; background-color: #ccdfe8;">
				ComfyUI -> Пакеты -> Extensions -> Manager -> setup ComfyUI-Manager
			</p>
			<p><b>HighresFix</b></p>
			<ul>
				<li>При её включении SD делит генерацию изображения на несколько стадий.</li>
			</ul>
			<p><b>Refiner</b></p>
			<ul>
				<li>Так как Refiner по сути это некоторое подобие img2img, мы можем использовать другой Checkpoint для доработки базового результата. Это бывает полезно если базовая модель генерит интересные стилистически, но не особо качественные, изображения. Refiner, за счёт другой модели, позволяет сохранять базовую форму и сюжет, дорабатывая при этом детали.</li>
			</ul>
			<p><b>Sampler</b></p>
			<ul>
				<li>Euler A - гладкий идеализированный результат (50+ шагов для хорошего качества)</li>
				<li>DPM++ 2M Karras - семплер общего назначения, быстро даёт качественный результат (20 шагов), обладает хорошей вариативностью</li>
				<li>Heun - Хорошо подходит для постобработки и добавления микродеталей.</li>
				<li>Все остальные семплеры либо более медленные, либо дают более странные результаты.</li>
			</ul>
			<p><b>CFGScale</b></p>
			<ul>
				<li>Этот параметр определят на сколько сильно SD "старается" сгенерить то, что вам нужно.</li>
				<li>Стандартное значение = 7.</li>
				<li>Если мы понижаем CFGScale, картинка становится менее сатурированной, а сюжет - более расплывчатым. Если повышаем - цветность повышается, некоторые детали становятся более нарочитыми, появляется "пережжённость" изображения.</li>
				<li>В основном этот параметр не изменяется, так как это не приводит к качественно лучшим результатам. Крутят его тогда, когда отдельно взятый Checkpoint генерит слишком насыщенные картинки, либо при использовании изотерических семплеров (LCM) или extension'ов.</li>
			</ul>
			<p><b>Img2Img</b></p>
			<p>SD позволяет генерить изображение на основе других изображений. Для этого предусмотрен отдельный режим img2img.</p>
			<p>На вход, помимо промпта подаётся изображение. И в зависимости от силы перерисовки denoising strength, изображение меняется в нужную сторону.</p>
			<ul>
				<li>0.0 - 0.2 - Изменяются самые мелкие детали</li>
				<li>0.2 - 0.4 - Меняется качество "рисовки", и среднемелкие детали</li>
				<li>0-4 - 0.5 - Крупные изменения с сохранением всех основных концепций оригинальной картинки</li>
				<li>0-5 - 0.65 - Работа на тему, с сохранением композиции</li>
				<li>0.65 - 1.0 - Что-то отдалённо напоминающее оригинальное изображение</li>
			</ul>
			<p>По этому при перерисовке больших изображений имеет смысл сильнее задирать Denoising Strength.</p>
			<p><b>Inpaint</b></p>
			<p>При генерации в img2img можно ограничить область действия SD.</p>
			<p>Это нужно в тех случаях, когда необходимо:</p>
			<ul>
				<li>изменить лицо</li>
				<li>перерисовать одежду</li>
				<li>добавить элементов окружения в нужное место</li>
				<li>починить анатомию</li>
				<li>изменить текстуру предмета</li>
				<li>детализировать глаза</li>
			</ul>
			<p><b>Only Masked vs Whole Picture</b></p>
			<p>Only Masked - перерисовывает только те пиксели, что попадают в маску. Это бывает полезно для увеличения детализации на отдельном участке большого изображения.  </p>
			<p><b>Whole Picture</b> - перерисовывает картинку целиком.</p>
			<p><b>Loopback</b></p>
			<p>Когда нужно сгенерить качественно другое изображение, но композиция, поза и суть должны остаться оригинальными.</p>
			<p><b>XYZ Plot</b></p>
			<p><b>SDUpcale</b></p>
			<p><b>CADS</b></p>
			<p><b>Kohya Highres.fix</b></p>
			<p><b>ADetailer</b></p>
			<p><b>Dynamic Prompts</b></p>
			<p><b>ControlNet</b> - это попытка глубоко забраться в мозги SD и склонить его в генерацию того что нужно.</p>
			<p><b>IP Adapter</b></p>
			<p>
				<pre>Installation on Windows 10/11 with NVIDIA GPUs
This is the simplest and most straightfoward installation.
Download the zip file sd.webui.zip from this link: v1.0.0-pre and extract its contents.
Run update.bat.
Run run.bat.
Installation on Windows with AMD GPU
Follow the official instructions. Note that performance will not be as good as if you had an NVIDIA GPU.
Place the model in the Web UI folder
When the AUTOMATIC1111 installation is complete, you will have a folder somewhere on your hard drive called stable-diffusion-webui.
After your model file (.cpkt or .safetensor) is finished downloading, place it in the folder stable-diffusion-webui/models/Stable-diffusion
(stable-diffusion-webui is folder containing the WebUI you downloaded in the first step)
Google Colab setups normally require you upload this model to Google Drive and connect the notebook to Google Drive.
Start the WebUI
Windows: double-click webui-user.bat to start
Linux: run webui-user.sh to start
Mac: run ./webui.sh to start
You will know it’s ready when you see the line Running on local URL: http://127.0.0.1:7860
Go to this address in your web browser: http://127.0.0.1:7860
By now you can already start prompting and getting results.
However, for best results, I recommend picking up some LoRAs.
What are LoRAs? They’re smaller models trained on a specific subject (such as an art style, a character, a body type, a sex act etc).
Basically you use them in combination with the checkpoint model you downloaded in the first step. There are LoRAs for everything, and people keep on making more of them.
Thankfully, LoRAs have much smaller file sizes than checkpoint models. You can use as many LoRAs as you want in a single prompt.
You activate your LoRAs by adding a special phrase inside your prompt. We’ll get to that in a bit.
I’m going to download the following popular LoRA breastInClass for this example, a LoRA that creates bodies with better proportions.
After downloading this .safetensor file, place it in the folder stable-diffusion-webui/models/Lora</pre>
			</p>
			<hr>
			<div class="images">
				<img src="../image/Neural-Networks/ollma-commands.png" width="393px"/>
				<img src="../image/Neural-Networks/Stable-diffusion-gui-settings.png" width="1000px"/>
				<img src="../image/Neural-Networks/Stable-diffusion-gui-convert-1.png" width="1000px"/>
				<img src="../image/Neural-Networks/Stable-diffusion-gui-convert-2.png" width="1000px"/>
				<img src="../image/Neural-Networks/Text-to-image-generation-with-ControlNet-Conditioning.png" width="1000px"/>
				<img src="../image/Neural-Networks/neural-image.png" width="1000px"/>
				<img src="../image/Neural-Networks/neural-networks-show.png" width="500px"/>
			</div>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
		</div>
		<div class="content">
			<p><a name="part2"></a></p>
			<h2>2. Нейронные сети на Android / IOS</h2>
			<p>Для примера будем использовать смартфон <b>&laquo;Xiomi Redmi 14C&raquo;</b>, который имеет следующие наиболее важные характеристики:</p>
			<ul>
				<li>Оперативная память: <b>8.0 ГБ</b></li>
				<li>Процессор: <b>8 ядер, макс. 2.0 ГГц</b></li>
				<li>Версия Android: <b>15 ...</b></li>
				<li>Внутренняя памаять: <b>119.2 ГБ / 256 ГБ</b></li>
			</ul>
			<p><a name="part2.1"></a></p>
			<h3>1.1 Интерфейсы.</h3>
			<p>И в <b>Android</b>, и в <b>IOS</b> репозиториях содержутся следующие приложения для скачивания и установки:</p>
			<ul>
				<li><b>PocketPal AI</b></li>
				<li><b>ChatterUI</b></li>
				<li><b>LLM Farm</b></li>
				<li><b>Private LLM</b> / <b>Open LLM</b></li>
			</ul>
			<p><b>Какие из них ещё придётся настраивать?</b></p>
			<p>Возьмем в пример следующие модели с huggingface и ollama: <i>Mykes/medicus, gemma3, codegemma, codellama, dolphin-mistral, llava, falcon, mistral, llama2-uncensored, reefer/erplegend, gdisney/neural-chat-uncensored, reefer/erphermesl3, jimscard/adult-film-screenwriter-nsfw, Llama-2, Llama-3.2, nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF, pygmalion (PygmalionAI)</i>.</p>
			<p>Из них на <b>Redmi 14C</b> с <u>PocketPal AI</u> и <u>ChatterUI</u> лучше запускать модели с размером <b>до 3-4 млрд параметров</b> и <b>с квантизацией 4-bit или 8-bit</b> в формате <b>GGUF/GGML</b>.</p>
			<p>Например:</p>
			<ul>
				<li>nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF — 3B модель с GGUF.</li>
				<li>pygmalion (PygmalionAI) — часто доступна в облегчённых версиях.</li>
				<li>falcon, mistral — есть версии с квантизацией, но лучше выбирать меньшие варианты (7B и ниже).</li>
				<li>llama2-uncensored, Llama-2, Llama-3.2 — для Redmi 14C лучше использовать 3B или 7B версии с квантизацией.</li>
				<li>codellama, codegemma — кодовые модели, обычно тяжелее, но с квантизацией могут работать на 7B и ниже.</li>
			</ul>
			<p><b>Интерфейсы и настройки, пояснения.</b></p>
			<ul>
				<li>PocketPal AI и ChatterUI — требуют минимальной настройки, если модель уже в GGUF с квантизацией. Для моделей без готовой квантизации придётся конвертировать и оптимизировать.</li>
				<li>LLM Farm — требует больше ручной настройки, особенно для мобильных устройств.</li>
				<li>Private LLM / Open LLM — часто требуют настройки под конкретное железо, оптимизации квантизации и формата.</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part2.2"></a></p>
			<h3>1.2 Параметры.</h3>
			<p><b>Общие параметры для всех интерфейсов.</b></p>
			<ul>
				<li><b>Модель (Model path)</b></li>
				<p>Укажите путь к модели в формате GGUF/GGML с квантизацией 4-bit или 8-bit, например: <i>models/nidum-llama-3.2-3b.gguf</i><br>Это позволит загрузить оптимизированную модель, подходящую для ограниченных ресурсов Redmi 14C.</p>
				<li><b>Квантизация (Quantization)</b></li>
				<p>
					Укажите тип квантизации, например:<i>--quantize 4bit</i> или <i>--quantize 8bit</i><br>
					Это уменьшит размер модели и нагрузку на CPU/GPU.
				</p>
				<li><b>Размер контекста (Context size)</b></li>
				<p>
					Например:<i>--ctx_size 2048</i> или <i>--ctx_size 4096</i><br>
					Чем больше контекст, тем больше памяти требуется. Для Redmi 14C лучше ограничиться 2048 токенами, чтобы избежать тормозов.
				</p>
				<li><b>Потоки CPU (Threads)</b></li>
				<p>
					Например: <i>--threads 4</i><br>
					Redmi 14C обычно имеет 4-8 ядер, но для стабильности лучше ограничить количество потоков до 4.
				</p>
				<li><b>Использование GPU (если поддерживается)</b></li>
				<p>
					Если интерфейс поддерживает GPU-ускорение (например, через Vulkan или OpenCL), включите его: <i>--use_gpu true</i><br>
					Но на Redmi 14C GPU слабый, поэтому часто лучше использовать CPU.
				</p>
			</ul>
			<p><b>Специфичные параметры для PocketPal AI и ChatterUI</b></p>
			<ul>
				<li><b>Формат модели</b></li>
				<p>
					Например:<br>
					PocketPal AI и ChatterUI лучше работают с GGUF/GGML. Убедитесь, что модель конвертирована в этот формат.
				</p>
				<li><b>Оптимизация памяти</b></li>
				<p>
					В PocketPal AI можно включить опцию: <i>--low_vram true</i><br>
					Это уменьшит использование оперативной памяти.
				</p>
				<li><b>Темп генерации (Temperature)</b></li>
				<p>
					Например: <i>--temperature 0.7</i><br>
					Регулирует креативность ответов. Для более стабильных ответов ставьте 0.6-0.8.
				</p>
				<li><b>Максимальная длина ответа (Max tokens)</b></li>
				<p>
					Например: <i>--max_tokens 256</i><br>
					Ограничивает длину генерируемого текста, чтобы не перегружать устройство.
				</p>
			</ul>
			<p><b>Параметры для LLM Farm и Private/Open LLM</b></p>
			<ul>
				<li><b>Путь к модели и квантизация — как в общих параметрах</b></li>
				<li><b>Параметры запуска</b></li>
				<p>
					В LLM Farm часто нужно указывать дополнительные параметры запуска модели, например:<br>
					<i>--use_mlock true</i> — блокирует модель в памяти, чтобы избежать свопа (если хватает RAM).<br>
					<i>--batch_size 8</i> — размер батча для генерации, уменьшайте для экономии ресурсов.
				</p>
				<li><b>Настройка логирования</b></li>
				<p>
					Для отладки полезно включить подробный лог: <i>--verbose true</i>
					
				</p>
				<li><b>Оптимизация под мобильные устройства</b></li>
				<p>
					В некоторых случаях нужно вручную конвертировать модель с помощью инструментов типа llama.cpp с параметрами:<br>
					<i>./quantize model.bin model.gguf 4</i> — для 4-битной квантизации.
				</p>
			</ul>
			<p><b>Пояснения к параметрам</b></p>
			<ul>
				<li><b>--quantize</b> Снижает размер модели и нагрузку на устройство.</li>
				<li><b>--ctx_size</b> Размер окна контекста, влияет на память и качество.</li>
				<li><b>--threads</b> Количество CPU потоков для обработки.</li>
				<li><b>--use_gpu</b> Включение GPU-ускорения (если поддерживается).</li>
				<li><b>--low_vram</b> Оптимизация использования памяти для слабых устройств.</li>
				<li><b>--temperature</b> Контролирует креативность и разнообразие ответов.</li>
				<li><b>--max_tokens</b> Максимальная длина генерируемого текста.</li>
				<li><b>--use_mlock</b>Блокирует модель в RAM, чтобы избежать свопа.</li>
				<li><b>--batch_size</b> Размер батча для генерации, влияет на скорость и память.</li>
				<li><b>--verbose</b> Включает подробный лог для отладки.</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part2.3"></a></p>
			<h3>1.3 Модели.</h3>
			<p>Модели с huggingface и ollama:</p>
			<ul>
				<li>Mykes/medicus</li>
				<li>gemma3</li>
				<li>codegemma</li>
				<li>codellama</li>
				<li>dolphin-mistral</li>
				<li>llava</li>
				<li>falcon</li>
				<li>mistral</li>
				<li>llama2-uncensored</li>
				<li>reefer/erplegend</li>
				<li>gdisney/neural-chat-uncensored</li>
				<li>reefer/erphermesl3</li>
				<li>jimscard/adult-film-screenwriter-nsfw</li>
				<li>Llama-2</li>
				<li>Llama-3.2</li>
				<li>nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF</li>
				<li>pygmalion (PygmalionAI)</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
			<hr>
			<p><a name="part2.4"></a></p>
			<h3>1.4 Итоги.</h3>
			<p>Для <b>Xiaomi Redmi 14C</b> при настройке интерфейсов <b>LLM</b> стоит ориентироваться на:</p>
			<ul>
				<li>Использование моделей с квантизацией <u>4-bit</u> или <u>8-bit</u> в формате <u>GGUF/GGML</u>.</li>
				<li>Ограничение контекста <u>до 2048 токенов</u>.</li>
				<li>Установка количества <u>потоков CPU в 4</u>.</li>
				<li>Включение оптимизаций памяти (<u>--low_vram</u>).</li>
				<li>Контроль <u>температуры</u> и <u>максимальной длины ответа</u> для баланса качества и производительности.</li>
			</ul>
			<p><a href="#oglavlenie">Перейти к оглавлению</a>.</p>
		</div>
		<div class="content">
			<p>Ну а сегодня на этом всё. Всем Добра и Удачи!</p>
		</div>	
		<div class="about">
			<p>Copyright &copy; 26.10.2025 by <a href="mailto:maximalis171091@yandex.ru">Mikhail Artamonov</a></p>
		</div>
		</br>
	</div>
	<script type="text/javascript">$('.fz__minimized').litezoom({speed:400, viewTitle:true});</script>
</body>
</html>
